[
  
  {
    "title"    : "Installing OpenShift in Manual Credentials Mode",
    "category" : "",
    "tags"     : " aws, cloud, kubernetes, openshift",
    "url"      : "/blog/2021/04/20/manual-credentials-mode.html",
    "date"     : "April 20, 2021",
    "excerpt"  : "\nA step-by-step walk-through of installing OpenShift (on AWS) with Installer-provisioned Infrastructure using manual credentials mode.\n\n\nwhy\n\n\nOpenShift comes with a Cloud Credentials Operator that manages the credentials used to operate the clust...",
  "content"  : "\nA step-by-step walk-through of installing OpenShift (on AWS) with Installer-provisioned Infrastructure using manual credentials mode.\n\n\nwhy\n\n\nOpenShift comes with a Cloud Credentials Operator that manages the credentials used to operate the cluster. However, many organizations have established processes and controls to manage IAM credentials, and prefer (or mandated) to utilize their existing processes and mechanisms for consistency/compliance and various reasons.\n\n\n\n\ngetting the installer\n\n\nDownload the OpenShift installer here. You may need to create a Red Hat account if you don&amp;#8217;t already have one. This is also where you download your pull secret that you&amp;#8217;ll need later in the installation process.\n\n\n\n\ninstallation credential\n\n\nWe need to run the installer using an AWS account with the necessary permissions to provision VPC and other resources. This can be an administrator account or and account provisioned with the necessary permissions as documented here.\n\n\nI have an example policy defined on github gist. We create an IAM user with this policy attached using the AWS CLI, or web console.\n\n\n\n\nenter terraform\n\n\nIn this article, I&amp;#8217;m using Terraform to create the installation credential, as well as the other credentials and artifacts required by OpenShift. You can download Terraform from here.\n\n\nClone my terraform config like so:\n\n\n\ngit clone https://github.com/naikoob/openshift-aws-terraform.git\n\n\n\nCd into the cloned project and initialize the terraform working directory:\n\n\n\ncd openshift-aws-terraform\nterraform init\n\n\n\nWe can review what gets created by the config with\n\n\n\nterraform plan\n\n\n\nFinally, apply the changes with 7\n\n\n\nterraform apply\n\n\n\nWhen terraform completes, the following set of users will be created:\n\n\n\n\n\n\n\nthe following policies are also created\n\n\n\n\n\n\n\nFinally, the following files are generated:\n\n\n\n\n\n\n\n\n\ninstalling openshift\n\n\nOne of the generated files above is the &quot;installer-credentials&quot;. This is the credential file for the openshift-installer user created by the Terraform config above. Append this to your AWS CLI credentials file (typically ~/.aws/credentials).\n\n\nMake sure the openshift-installer profile is used. On MacOS or Linux, do this:\n\n\n\nexport AWS_PROFILE=openshift-installer\n\n\n\nNext we can generate the installation configuration with\n\n\n\n./openshift-installer create install-config --dir config\n\n\n\nYou will be prompted for a few inputs, including your pull-secret downloaded from here earlier.\n\n\n\n\n\n\n\nconfigure credentials mode\n\nEdit the generated install-config to add credentialsMode as follows:\n\n\n\n apiVersion: v1\n baseDomain: demo.xcdc.io\n credentialsMode: Manual (1)\n compute:\n - architecture: amd64\n   hyperthreading: Enabled\n   name: worker\n   platform: {}\n   replicas: 3\n controlPlane:\n   architecture: amd64\n   hyperthreading: Enabled\n   name: master\n   platform: {}\n   replicas: 3\n metadata:\n   creationTimestamp: null\n   name: ocp47\n   # .... more config reducted\n\n\n\n\n\n1\nadd this line to select Manual credential mode.\n\n\n\n\n\ngenerate manifests\n\nNext, we generate the manifests with:\n\n\n\n./openshift-install create manifests --dir config\n\n\n\nThis will generate the manifest files under the config directory. We will copy the secrets yamls generated by terraform config (under generated-files/secrets subdirectory) into the config/manifests directory. The resulting directory structure should look like this:\n\n\n\n\n\n\n\nYou will notice that the install-config generated earlier will be removed. This is normal, so don&amp;#8217;t be alarmed.\n\n\n\ncreate cluster\n\nWe are now ready to create the cluster with:\n\n\n\n./openshift-install create cluster --dir config --log-level debug\n\n\n\nYour OpenShift cluster should be up and running in about 40 minutes, and you can login to the OpenShift console using the console url and credentials created by the installer.\n\n\nFinally, we can confirm that the cluster is indeed operating in Manual credentials mode by inspecting the cluster-config-v1 configmap in the kube-system namespace using the web console, or from the oc command line like so:\n\n\n\noc get cm -n kube-system cluster-config-v1 -o yaml | grep -i credentialsmode\n\n\n\n\n"
} ,
  
  {
    "title"    : "OpenShift and (AWS) IAM",
    "category" : "",
    "tags"     : " aws, cloud, kubernetes, openshift",
    "url"      : "/blog/2021/03/19/openshift-cloud-credentials.html",
    "date"     : "March 19, 2021",
    "excerpt"  : "\nI wrote about the AWS resources created by OpenShift installer in my earlier post. In this post, let&#39;s take a closer look at IAM credentials used by OpenShift on AWS.\n\n\ninstallation credentials\n\n\nThe simplest way to install OpenShift on AWS is to...",
  "content"  : "\nI wrote about the AWS resources created by OpenShift installer in my earlier post. In this post, let&#39;s take a closer look at IAM credentials used by OpenShift on AWS.\n\n\ninstallation credentials\n\n\nThe simplest way to install OpenShift on AWS is to make use of the IPI (Installer Provisioned Infrastructure) method. With IPI, the installation process will provision a new VPC and other infrastructure resources, and then install OpenShift on top of it. It means that we need to use an account with the necessary privileges to create VPC, load balancers, etc for installation. We can use an adminstrator account for this, or, we can refer to the documentation for more specific permission requirements.\n\n\nI also have a ready to use policy file here (for OpenShift 4.7).\n\n\nNote that the credential used during installation is stored in your cluster as a Kubernetes secret named aws-creds in the kube-system namespace:\n\n\n\n\n\n\n\nFor OpenShift on AWS, we can delete this secret post installation. However, it will need to be reinstated when we need to perform cluster upgrades, as documented here. Alternatively, we can go into the AWS console and deactivate the corresponding access key and re-activate it when you want to perform a cluster upgrade.\n\n\n\n\nopenshift created roles\n\n\nAs part of installation, OpenShift create roles for the bootstrap, master and worker nodes. The bootstrap role, and  associated policy is only used during the installation, and are deleted when the installation completes.\n\n\nThe master role&amp;#8217;s policy allows master nodes to configure load balancers as workload gets deployed/removed from the cluster, while the worker policy only allows the worker nodes to retrieve it&amp;#8217;s own metadata, and region information.\n\n\nThese roles do not have any IAM permissions, and therefore are not able to create or modify access rights.\n\n\n\n\ncloud credentials operator (cco)\n\n\nPost installation, OpenShift interacts with the underlying cloud infrastructure provider to perform operations such as provisioning additional worker nodes, or configure load balancers as workloads get provisioned and orchestrated on the platform. To do this, OpenShift needs the necessary perimissions on the underlying infrastructure. The Cloud Credential Operator (CCO) is responsible for managing these credentials. The CCO operates on CredentialsRequest custom resource, and behaves differently based on the credentialMode setting.\n\n\ncco modes\n\nBy default, CCO operates in the Mint mode. In this mode, CCO creates (mints) new credentials for components in the cluster. This allows it to create credentials with very specific permissions for the requesting component. However, this also means you need to provide an account with rights to create IAM users and roles to the CCO, which often raise concerns with enterprise security teams.\n\n\nAn alternative is the Manual mode, where the CCO takes a backseat and user has to manage all the different credentials used by OpenShift. For version 4.7 of OpenShift, there are 5 credentials to manage as described in my previous post.\n\n\nA third, Passthrough mode, is available for OpenShift on AWS since version 4.5.8. In this mode, a single IAM credential with permissions to perform the different cluster functions is used for all components. Crucially, there are no permission to create or modify IAM configuration when using Passthrough mode, which should set security team at ease.\n\n\n\ncredentials request\n\nCredentialsRequest is a custom Kubernetes resource (CRD) that describes the desired state of cloud credentials. Using the OpenShift command line client, we can obtain the credentialsRequest for the target cloud like so (version 4.7.2 on AWS):\n\n\n\noc adm release extract quay.io/openshift-release-dev/ocp-release:4.7.2-x86_64 --credentials-requests --cloud=aws\n\n\n\nThe result should be as follows:\n\n\n\n---\napiVersion: cloudcredential.openshift.io/v1\nkind: CredentialsRequest\nmetadata:\n  annotations:\n    exclude.release.openshift.io/internal-openshift-hosted: &quot;true&quot;\n    include.release.openshift.io/self-managed-high-availability: &quot;true&quot;\n  name: cloud-credential-operator-iam-ro\n  namespace: openshift-cloud-credential-operator\nspec:\n  providerSpec:\n    apiVersion: cloudcredential.openshift.io/v1\n    kind: AWSProviderSpec\n    statementEntries:\n    - action:\n      - iam:GetUser\n      - iam:GetUserPolicy\n      - iam:ListAccessKeys\n      effect: Allow\n      resource: &#39;*&#39;\n  secretRef:\n    name: cloud-credential-operator-iam-ro-creds\n    namespace: openshift-cloud-credential-operator\n---\napiVersion: cloudcredential.openshift.io/v1\nkind: CredentialsRequest\nmetadata:\n  annotations:\n    include.release.openshift.io/ibm-cloud-managed: &quot;true&quot;\n    include.release.openshift.io/self-managed-high-availability: &quot;true&quot;\n    include.release.openshift.io/single-node-developer: &quot;true&quot;\n  labels:\n    controller-tools.k8s.io: &quot;1.0&quot;\n  name: openshift-image-registry\n  namespace: openshift-cloud-credential-operator\nspec:\n  providerSpec:\n    apiVersion: cloudcredential.openshift.io/v1\n    kind: AWSProviderSpec\n    statementEntries:\n    - action:\n      - s3:CreateBucket\n      - s3:DeleteBucket\n      - s3:PutBucketTagging\n      - s3:GetBucketTagging\n      - s3:PutBucketPublicAccessBlock\n      - s3:GetBucketPublicAccessBlock\n      - s3:PutEncryptionConfiguration\n      - s3:GetEncryptionConfiguration\n      - s3:PutLifecycleConfiguration\n      - s3:GetLifecycleConfiguration\n      - s3:GetBucketLocation\n      - s3:ListBucket\n      - s3:GetObject\n      - s3:PutObject\n      - s3:DeleteObject\n      - s3:ListBucketMultipartUploads\n      - s3:AbortMultipartUpload\n      - s3:ListMultipartUploadParts\n      effect: Allow\n      resource: &#39;*&#39;\n  secretRef:\n    name: installer-cloud-credentials\n    namespace: openshift-image-registry\n---\napiVersion: cloudcredential.openshift.io/v1\nkind: CredentialsRequest\nmetadata:\n  annotations:\n    include.release.openshift.io/ibm-cloud-managed: &quot;true&quot;\n    include.release.openshift.io/self-managed-high-availability: &quot;true&quot;\n    include.release.openshift.io/single-node-developer: &quot;true&quot;\n  labels:\n    controller-tools.k8s.io: &quot;1.0&quot;\n  name: openshift-ingress\n  namespace: openshift-cloud-credential-operator\nspec:\n  providerSpec:\n    apiVersion: cloudcredential.openshift.io/v1\n    kind: AWSProviderSpec\n    statementEntries:\n    - action:\n      - elasticloadbalancing:DescribeLoadBalancers\n      - route53:ListHostedZones\n      - route53:ChangeResourceRecordSets\n      - tag:GetResources\n      effect: Allow\n      resource: &#39;*&#39;\n  secretRef:\n    name: cloud-credentials\n    namespace: openshift-ingress-operator\n---\napiVersion: cloudcredential.openshift.io/v1\nkind: CredentialsRequest\nmetadata:\n  annotations:\n    include.release.openshift.io/ibm-cloud-managed: &quot;true&quot;\n    include.release.openshift.io/self-managed-high-availability: &quot;true&quot;\n    include.release.openshift.io/single-node-developer: &quot;true&quot;\n  name: aws-ebs-csi-driver-operator\n  namespace: openshift-cloud-credential-operator\nspec:\n  providerSpec:\n    apiVersion: cloudcredential.openshift.io/v1\n    kind: AWSProviderSpec\n    statementEntries:\n    - action:\n      - ec2:AttachVolume\n      - ec2:CreateSnapshot\n      - ec2:CreateTags\n      - ec2:CreateVolume\n      - ec2:DeleteSnapshot\n      - ec2:DeleteTags\n      - ec2:DeleteVolume\n      - ec2:DescribeInstances\n      - ec2:DescribeSnapshots\n      - ec2:DescribeTags\n      - ec2:DescribeVolumes\n      - ec2:DescribeVolumesModifications\n      - ec2:DetachVolume\n      - ec2:ModifyVolume\n      effect: Allow\n      resource: &#39;*&#39;\n  secretRef:\n    name: ebs-cloud-credentials\n    namespace: openshift-cluster-csi-drivers\n---\napiVersion: cloudcredential.openshift.io/v1\nkind: CredentialsRequest\nmetadata:\n  annotations:\n    exclude.release.openshift.io/internal-openshift-hosted: &quot;true&quot;\n    include.release.openshift.io/self-managed-high-availability: &quot;true&quot;\n  labels:\n    controller-tools.k8s.io: &quot;1.0&quot;\n  name: openshift-machine-api-aws\n  namespace: openshift-cloud-credential-operator\nspec:\n  providerSpec:\n    apiVersion: cloudcredential.openshift.io/v1\n    kind: AWSProviderSpec\n    statementEntries:\n    - action:\n      - ec2:CreateTags\n      - ec2:DescribeAvailabilityZones\n      - ec2:DescribeDhcpOptions\n      - ec2:DescribeImages\n      - ec2:DescribeInstances\n      - ec2:DescribeSecurityGroups\n      - ec2:DescribeSubnets\n      - ec2:DescribeVpcs\n      - ec2:RunInstances\n      - ec2:TerminateInstances\n      - elasticloadbalancing:DescribeLoadBalancers\n      - elasticloadbalancing:DescribeTargetGroups\n      - elasticloadbalancing:RegisterInstancesWithLoadBalancer\n      - elasticloadbalancing:RegisterTargets\n      - iam:PassRole\n      - iam:CreateServiceLinkedRole\n      effect: Allow\n      resource: &#39;*&#39;\n    - action:\n      - kms:Decrypt\n      - kms:Encrypt\n      - kms:GenerateDataKey\n      - kms:GenerateDataKeyWithoutPlainText\n      - kms:DescribeKey\n      effect: Allow\n      resource: &#39;*&#39;\n    - action:\n      - kms:RevokeGrant\n      - kms:CreateGrant\n      - kms:ListGrants\n      effect: Allow\n      policyCondition:\n        Bool:\n          kms:GrantIsForAWSResource: true\n      resource: &#39;*&#39;\n  secretRef:\n    name: aws-cloud-credentials\n    namespace: openshift-machine-api\n\n\n\nYou&amp;#8217;ll see that five(5) credentialsRequest will be created for OpenShift on AWS.\n\n\n\nmint mode\n\nWhen operating in Mint (the default) mode, the Cloud Credentials Operator creates an IAM user of each credentialRequest.\n\n\nIn addition, a Kubernetes secret containing the AWS access keys will be created and managed by the operator based on the spec/secretRef section of the credentialsRequest.\n\n\nTo illustrate, the following users are created when I install OpenShift using Mint mode:\n\n\n\n\n\n\n\nLet&amp;#8217;s examine the credentialsRequest for the ingress operator:\n\n\n\napiVersion: cloudcredential.openshift.io/v1\nkind: CredentialsRequest\nmetadata:\n  annotations:\n    include.release.openshift.io/ibm-cloud-managed: &quot;true&quot;\n    include.release.openshift.io/self-managed-high-availability: &quot;true&quot;\n    include.release.openshift.io/single-node-developer: &quot;true&quot;\n  labels:\n    controller-tools.k8s.io: &quot;1.0&quot;\n  name: openshift-ingress\n  namespace: openshift-cloud-credential-operator\nspec:\n  providerSpec:\n    apiVersion: cloudcredential.openshift.io/v1\n    kind: AWSProviderSpec\n    statementEntries: (1)\n    - action:\n      - elasticloadbalancing:DescribeLoadBalancers\n      - route53:ListHostedZones\n      - route53:ChangeResourceRecordSets\n      - tag:GetResources\n      effect: Allow\n      resource: &#39;*&#39;\n  secretRef: (2)\n    name: cloud-credentials\n    namespace: openshift-ingress-operator\n\n\n\n\n\n1\nspecifies the required permission, this will be defined as inline policy for the IAM user\n\n\n2\nspecifies the namespace and name of the corresponding secret that will contain the access keys to this user\n\n\n\n\n\nmanual mode\n\nIn Manual mode, the user manages the cloud credentials instead of the Cloud Credentials Operator. This means we need to create the IAM users with the permissions as specified in the credentialsRequest above, along with the access keys and secrets during installation by following the instructions here.\n\n\nWe also should reconcile the permissions with credentialsRequests of target versions before performing cluster upgrades.\n\n\n\npassthrough mode\n\nIn Passthrough mode, the Cloud Credentials Operator does not create IAM users. Instead, the secrets contains the same AWS access keys used during installation. To rotate the access key, we update the aws-creds secret in the kube-system namespace. The CCO will they sync the other credential secrets with the same access keys specified in the aws-creds.\n\n\nThis implies that all the different components will be using the same credentials. Hence, the supplied credentials needs to have all the permissions requested by all the credentialsRequest.\n\n\nNote also that by default, kube-system/aws-creds contains the access keys used during installation. This contains permissions that are not required during normal operations in Passthrough mode, such as creating IAM users. To mitigate this, we can create 2 separate policies for installation and operations.\n\n\nAttach both policies to the user during installation like so:\n\n\n\n\n\n\n\nOnce installation is complete, remove the openshift-install-policy from the user.\n\n\nAlternatively, we can create a separate IAM user for operation, and change the kube-system/aws-creds secret to use the operation account post installation.\n\n\nIn my (albeit limited) testing, we can perform cluster upgrades with just the operations policy attached with Passthrough mode, although it is prudent to review the credentialsRequest for the target version for any change in permissions.\n\n\n\n\n\nwhich option&amp;#8217;s for me?\n\n\nSo which option should we take?\n\n\nMint mode offers the most convenience from operations point of view. However, I recommend that you delete/deactivate the kube-system/aws-cred access keys post installation, and only re-instating them when performing cluster upgrades.\n\n\nManual and Passthrough modes are suitable for organizations that have an established process of managing IAM credentials. In this case, I suggest extending that process to include creating/rotating of credentials secrets. When using Passthrough mode, it&amp;#8217;s a good idea to remove the IAM creation permissions after installation, as described above.\n\n\nFinally, do consider configuring alerts to detect any unplanned changes to IAM configuration.\n\n\n\n\nwhat&amp;#8217;s next?\n\n\nWe&amp;#8217;ll be able to use short lived tokens instead of access keys soon with manual mode using STS (Secure Token Service). This should improve the security posture and is currently in Technology Preview. I&amp;#8217;ll look into this operating mode in a future article.\n\n\n"
} ,
  
  {
    "title"    : "OpenShift IPI on AWS, for the paranoids",
    "category" : "",
    "tags"     : " aws, cloud, kubernetes, openshift",
    "url"      : "/blog/2021/01/28/openshift-on-aws.html",
    "date"     : "January 28, 2021",
    "excerpt"  : "\nThe simplest way to install OpenShift on AWS is to make use of the IPI (Installer Provisioned Infrastructure) method. With IPI, the installation process will provision the necessary VPC and other infrastructure resources, and then install OpenShi...",
  "content"  : "\nThe simplest way to install OpenShift on AWS is to make use of the IPI (Installer Provisioned Infrastructure) method. With IPI, the installation process will provision the necessary VPC and other infrastructure resources, and then install OpenShift onto the provisioned infrastructure. Sounds good, but to the curious (and paranoids), exactly what AWS resources were created by the installer? Let&#39;s take a look.\n\n\ninstallation inputs\n\n\n[UPDATE: 24-March-2021] - Red Hat OpenShift for AWS (ROSA) is now generally available!\n\n\n[UPDATE: 18-March-2021] - This article has been updated to use OpenShift version 4.7.2\n\n\nIf you have a Red Hat Developer&amp;#8217;s account (http://developers.redhat.com), just follow this link to download the OpenShift installer for AWS (IPI), as well as the pull secret &amp;#8594; https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n\n\nWe&amp;#8217;ll need to have the necessary AWS account (and access keys) to run the installer. We also need to prepare a SSH public key, and a Route 53 public hosted zone (the base domain). Once we have all these, we can create the cluster using\n\n\n\n./openshift-installer create cluster --dir=&amp;lt;working-dir&amp;gt;\n\n\n\nThe installer will prompt for a couple of information as shown below:\n\n\n\n\n\n\n\n&amp;#8230;&amp;#8203; and after around 40 minutes, you should get your OpenShift cluster.\n\n\n\n\nbut what actually happened?\n\n\nThe installation process uses terraform to provision the VPC and other AWS resources. EC2 instances with base OpenShift images are then launched to form the cluster and install the core OpenShift operators.\n\n\nPost installation, you&amp;#8217;ll see a few files created in the installation working directory (in my case ipi), including terraform.tfstate that describes the resources created by the installation process. In addition, there are also a few other IAM users being provisioned by OpenShift operators (e.g. the openshift-image-registry operator)\n\n\nHere&amp;#8217;s a graphical representation of the results:\n\n\n\n\n\n\n\nusers, roles\n\nLet&amp;#8217;s start with IAM resources (remember paranoid?). Head over to the AWS IAM console, we see the following IAM resources created:\n\n\n\n\n\n\n\n\n\nName*\nType\nCreated/Used By\n\n\n\n\naws-ebs-csi-driver-operator*\nUser\nCluster Storage Operator\n\n\ncloud-credential-operator-iam-ro\nUser\nCloud Credential Operator\n\n\nopenshift-image-registry\nUser\nCluster Image Registry Operator\n\n\nopenshift-ingress\nUser\nIngress Operator\n\n\nopenshift-machine-api-aws\nUser\nMachine API Operator\n\n\nmaster-role\nRole\nInstaller\n\n\nworker-role\nRole\nInstaller\n\n\n\n\n* there will be a prefix and suffix to the user/role names, for example `ocp46-pj42g-aws-ebs-csi-driver-operator-djdvq`\n\n\nEach of the user/role above has attached inline policy that grant access to necessary AWS resources. We can use AWS IAM permission boundaries to apply more restrictive permissions if desired. Do consult Red Hat&amp;#8217;s support if you plan to do this.\n\n\nFor instance, the policy for the image registry user is as follows:\n\n\n\n{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:CreateBucket&quot;,\n                &quot;s3:DeleteBucket&quot;,\n                &quot;s3:PutBucketTagging&quot;,\n                &quot;s3:GetBucketTagging&quot;,\n                &quot;s3:PutBucketPublicAccessBlock&quot;,\n                &quot;s3:GetBucketPublicAccessBlock&quot;,\n                &quot;s3:PutEncryptionConfiguration&quot;,\n                &quot;s3:GetEncryptionConfiguration&quot;,\n                &quot;s3:PutLifecycleConfiguration&quot;,\n                &quot;s3:GetLifecycleConfiguration&quot;,\n                &quot;s3:GetBucketLocation&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:HeadBucket&quot;,\n                &quot;s3:GetObject&quot;,\n                &quot;s3:PutObject&quot;,\n                &quot;s3:DeleteObject&quot;,\n                &quot;s3:ListBucketMultipartUploads&quot;,\n                &quot;s3:AbortMultipartUpload&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;iam:GetUser&quot;\n            ],\n            &quot;Resource&quot;: &quot;&amp;lt;registry-user-arn&amp;gt;&quot;\n        }\n    ]\n}\n\n\n\nWe can define a permission boundary like below to restrict operations like GetObject, PutObject to only the image registry S3 bucket (instead of any buckets in the account):\n\n\n\n{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;RegistryPermBoundary&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetBucketPublicAccessBlock&quot;,\n                &quot;s3:GetLifecycleConfiguration&quot;,\n                &quot;s3:ListBucketMultipartUploads&quot;,\n                &quot;s3:GetBucketTagging&quot;,\n                &quot;s3:PutBucketPublicAccessBlock&quot;,\n                &quot;s3:PutEncryptionConfiguration&quot;,\n                &quot;s3:PutObject&quot;,\n                &quot;s3:GetObject&quot;,\n                &quot;s3:GetEncryptionConfiguration&quot;,\n                &quot;s3:AbortMultipartUpload&quot;,\n                &quot;s3:PutBucketTagging&quot;,\n                &quot;s3:PutLifecycleConfiguration&quot;,\n                &quot;s3:GetBucketLocation&quot;,\n                &quot;s3:DeleteObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;&amp;lt;bucket-arn&amp;gt;&quot;,\n                &quot;&amp;lt;bucket-arn&amp;gt;/*&quot;\n            ]\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:HeadBucket&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;iam:GetUser&quot;\n            ],\n            &quot;Resource&quot;: &quot;&amp;lt;registry-user-arn&amp;gt;&quot;\n        },\n\n        {\n            &quot;Effect&quot;: &quot;Deny&quot;,\n            &quot;Action&quot;: &quot;s3:DeleteBucket&quot;,\n            &quot;Resource&quot;: &quot;*&quot;\n        }\n    ]\n}\n\n\n\nFirst create a policy from the above JSON, using\n\n\n\naws iam create-policy --policy-name &amp;lt;policy-name&amp;gt; --policy-document file://&amp;lt;path-to-policy-file&amp;gt;\n\n\n\nthen, attach the permission boundary to the image registry user:\n\n\n\naws iam put-user-permissions-boundary --permissions-boundary &amp;lt;policy-arn&amp;gt; --user-name &amp;lt;openshift-image-registry-user-name&amp;gt;\n\n\n\n\nvpc\n\nNext, we&amp;#8217;ll examine the VPC resources created. The installation process creates a public and a private subnet in each availability zone of selected AWS region. All master and worker nodes are launched in the private subnets. The nodes can reach out to the Internet via the NAT gateways that are launched in the public subnet in the same AZ.\n\n\nload balancers\n\nTwo separate network load balancers are provisioned to serve internal (ports 6443, 22623) and external (port 6443) API requests to the masters.\n\n\nA third, classic load balancer is provisioned to serve application ingress (ports 80, 443).\n\n\n\ndns\n\nThe installation process will create a private hosted zone for the cluster. In my case, the cluster name is ocp46 and my base domain is demo.xcdc.io, so the private hosted zone is ocp46.demo.xcdc.io. This hosted zone contains DNS entries for the internal and external API endpoints, as well as the wildcard entry for application ingress.\n\n\nDNS entries are also created in the supplied public hosted zone (demo.xcdc.io in my case), to publish the dns names for API end point (api.ocp4.demo.xcdc.io), as well as the wildcard entry (*.apps.ocp4.demo.xcdc.io) to the respective load balancers above.\n\n\n\nsecurity groups\n\nA security group is attached to the application ingress load balancer to allow only HTTP and HTTPS traffic.\n\n\nWorker nodes security group allows network traffic from the ingress load balancers, and selected traffic from the master nodes as well as other worker nodes below.\n\n\n\n\n\n\n\n\n\n\nports\nprotocols\nsource\ndescription\n\n\n\n\nall\nall\ningress load balancers\ningress traffic\n\n\nall\nicmp\nvpc\nICMP\n\n\nall\nESP (50)\nworkers, masters\nIPSec\n\n\n500,4500\nUDP\nworkers, masters\nIPSec\n\n\n22\ntcp\nvpc\nSSH\n\n\n4789\nudp\nworkers, masters\nVxlan packets\n\n\n6081\nudp\nworkers, masters\nGENEVE packets\n\n\n9000 - 9999\ntcp, udp\nworkers, masters\nInternal cluster communication\n\n\n10250\ntcp\nworkers, masters\nKubernetes kubelet, scheduler and controller manager\n\n\n30000 - 32767\ntcp, udp\nworkers, masters\nKubernetes ingress services\n\n\n\n\nMaster nodes security group allows selected traffic from workers and other master nodes:\n\n\n\n\n\n\n\n\n\n\nports\nprotocols\nsource\ndescription\n\n\n\n\nall\nicmp\nvpc\nICMP\n\n\nall\nESP (50)\nworkers, masters\nIPSec\n\n\n500,4500\nUDP\nworkers, masters\nIPSec\n\n\n22\ntcp\nvpc\nSSH\n\n\n2379 - 2380\ntcp\nmasters\netcd\n\n\n4789\nudp\nworkers, masters\nVxlan packets\n\n\n6081\nudp\nworkers, masters\nGENEVE packets\n\n\n6443\ntcp\nvpc\napi access\n\n\n6641 - 6642\ntcp\nworkers, masters\nOVN packets\n\n\n9000 - 9999\ntcp, udp\nworkers, masters\nInternal cluster communication\n\n\n10250\ntcp\nworkers, masters\nKubernetes kubelet, scheduler and controller manager\n\n\n10257\ntcp\nworkers, masters\nKubernetes kubelet, scheduler and controller manager\n\n\n10259\ntcp\nworkers, masters\nKubernetes kubelet, scheduler and controller manager\n\n\n22623\ntcp\nvpc\nmachine config service\n\n\n30000 - 32767\ntcp, udp\nworkers, masters\nKubernetes ingress services\n\n\n\n\n\n\nwhat&amp;#8217;s next?\n\nThis post describes the default infrastucture setup by OpenShift installer. It is possible to apply customizations such as CIDR ranges, machine instance types, etc. Red Hat&amp;#8217;s documentation has a good section on this here.\n\n\nIt is also possible to perform an OpenShift IPI installation into an existing VPC.\n\n\nLast but not least, OpenShift will be available as a managed service on AWS soon! Here&amp;#8217;s the announcement for Red Hat OpenShift Service on AWS.\n\n\n\n"
} ,
  
  {
    "title"    : "Developing for JBoss EAP",
    "category" : "",
    "tags"     : " eap, jboss, maven, wildfly",
    "url"      : "/blog/2020/10/22/eap-bom.html",
    "date"     : "October 22, 2020",
    "excerpt"  : "\nWhile developers gravitate towards cloud native languages and frameworks, there are still plenty of perfectly functioning Java code that are running in a lot of enterprises that may not be due for a rewrite/overhaul for one reason or another. The...",
  "content"  : "\nWhile developers gravitate towards cloud native languages and frameworks, there are still plenty of perfectly functioning Java code that are running in a lot of enterprises that may not be due for a rewrite/overhaul for one reason or another. These often run on a Java application server such Red Hat JBoss Enterprise Application Platform (JBoss EAP). Here is a simple tip on developing for JBoss EAP that may not be obvious for developers coming from other application servers.\n\n\nmaven\n\n\nDevelopers coming from other app servers may not know that Red Hat offers a Maven repository of Java components shipped with Red Hat middlewares. These also include Red Hat QC&amp;#8217;ed version of frameworks such as Hibernate, Slf4j, etc.\n\n\nSubscribers of JBoss EAP can add this Maven repo to their build by adding the following to their Maven settings.xml or in the project POM files.\n\n\n\n&amp;lt;repositories&amp;gt;\n    &amp;lt;repository&amp;gt;\n        &amp;lt;releases&amp;gt;\n            &amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;\n        &amp;lt;/releases&amp;gt;\n        &amp;lt;snapshots&amp;gt;\n            &amp;lt;enabled&amp;gt;false&amp;lt;/enabled&amp;gt;\n        &amp;lt;/snapshots&amp;gt;\n        &amp;lt;id&amp;gt;jboss-enterprise-maven-repository&amp;lt;/id&amp;gt;\n        &amp;lt;url&amp;gt;https://maven.repository.redhat.com/ga/&amp;lt;/url&amp;gt;\n    &amp;lt;/repository&amp;gt;\n&amp;lt;/repositories&amp;gt;\n&amp;lt;pluginRepositories&amp;gt;\n    &amp;lt;pluginRepository&amp;gt;\n        &amp;lt;releases&amp;gt;\n            &amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;\n        &amp;lt;/releases&amp;gt;\n        &amp;lt;snapshots&amp;gt;\n            &amp;lt;enabled&amp;gt;false&amp;lt;/enabled&amp;gt;\n        &amp;lt;/snapshots&amp;gt;\n        &amp;lt;id&amp;gt;jboss-enterprise-maven-repository&amp;lt;/id&amp;gt;\n        &amp;lt;url&amp;gt;https://maven.repository.redhat.com/ga/&amp;lt;/url&amp;gt;\n    &amp;lt;/pluginRepository&amp;gt;\n&amp;lt;/pluginRepositories&amp;gt;\n\n\n\nYou can find a sample settings.xml here.\n\n\n\n\nbom\n\n\nIn addition, Red Hat supplies Bill of Material (BOM) POMs that aggregates the different libaries used in a product so that they can be consumed easily.\n\n\nSpecifically, for JBoss EAP, the following BOMs are provided:\n\n\n\n\n\n\n\n\nBOM Artifact ID\nUse Case\n\n\n\n\neap-runtime-artifacts\nSupported JBoss EAP runtime artifacts.\n\n\njboss-eap-jakartaee8\nSupported JBoss EAP Jakarta EE 8 APIs plus additional JBoss EAP API JARs.\n\n\njboss-eap-jakartaee8-with-spring4\njboss-eap-jakartaee8 plus recommended Spring 4 versions.\n\n\njboss-eap-jakartaee8-with-tools\njboss-eap-jakartaee8 plus development tools such as Arquillian.\n\n\n\n\nSee the documentation here for details.\n\n\nTo make use of these BOMs, add a dependencyManagement entry in the project&amp;#8217;s POM file:\n\n\n\n&amp;lt;dependencyManagement&amp;gt;\n    &amp;lt;dependencies&amp;gt;\n        &amp;lt;!--\n            importing the jakartaee8-with-tools BOM adds specs and other useful\n            artifacts as managed dependencies\n        --&amp;gt;\n        &amp;lt;dependency&amp;gt;\n            &amp;lt;groupId&amp;gt;org.jboss.bom&amp;lt;/groupId&amp;gt;\n            &amp;lt;artifactId&amp;gt;jboss-eap-jakartaee8-with-tools&amp;lt;/artifactId&amp;gt;\n            &amp;lt;version&amp;gt;${version.jboss.eap}&amp;lt;/version&amp;gt;\n            &amp;lt;type&amp;gt;pom&amp;lt;/type&amp;gt;\n            &amp;lt;scope&amp;gt;import&amp;lt;/scope&amp;gt;\n        &amp;lt;/dependency&amp;gt;\n    &amp;lt;/dependencies&amp;gt;\n&amp;lt;/dependencyManagement&amp;gt;\n\n&amp;lt;properties&amp;gt;\n    &amp;lt;version.jboss.eap&amp;gt;7.3.3.GA&amp;lt;/version.jboss.eap&amp;gt;\n&amp;lt;/properties&amp;gt;\n\n\n\nWith this in place, we can refer to dependencies that are contained in the BOM without having to specify the version:\n\n\n\n&amp;lt;dependencies&amp;gt;\n    &amp;lt;!-- Import the CDI API, we use provided scope as the API is included in JBoss EAP --&amp;gt;\n    &amp;lt;dependency&amp;gt;\n        &amp;lt;groupId&amp;gt;jakarta.enterprise&amp;lt;/groupId&amp;gt;\n        &amp;lt;artifactId&amp;gt;jakarta.enterprise.cdi-api&amp;lt;/artifactId&amp;gt;\n        &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;\n    &amp;lt;/dependency&amp;gt;\n    &amp;lt;!--\n        Import the Common Annotations API (JSR-250), we use provided\n        scope as the API is included in JBoss EAP.\n    --&amp;gt;\n    &amp;lt;dependency&amp;gt;\n        &amp;lt;groupId&amp;gt;org.jboss.spec.javax.annotation&amp;lt;/groupId&amp;gt;\n        &amp;lt;artifactId&amp;gt;jboss-annotations-api_1.3_spec&amp;lt;/artifactId&amp;gt;\n        &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;\n    &amp;lt;/dependency&amp;gt;\n    &amp;lt;!-- Import the JAX-RS API, we use provided scope as the API is included in JBoss EAP. --&amp;gt;\n    &amp;lt;dependency&amp;gt;\n        &amp;lt;groupId&amp;gt;org.jboss.spec.javax.ws.rs&amp;lt;/groupId&amp;gt;\n        &amp;lt;artifactId&amp;gt;jboss-jaxrs-api_2.1_spec&amp;lt;/artifactId&amp;gt;\n        &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;\n    &amp;lt;/dependency&amp;gt;\n&amp;lt;/dependencies&amp;gt;\n\n\n\nNotice we can use the provided scope to make use of the JARs included in JBoss EAP and reduce the size of our deployables.\n\n\nWe can organize this in parent/child projects POMs as well, as found in the sample project here.\n\n\nThis approach ensure we use of as much of the Red Hat tested/certified JARs as possible, and take advantage of any bug/security fixes that Red Hat provides.\n\n\nMake full use of your Red Hat subscription :-)\n\n\n"
} ,
  
  {
    "title"    : "Sayonara, Prezto. Hello: dotfiles",
    "category" : "",
    "tags"     : " command line, stow, zsh",
    "url"      : "/blog/2020/10/02/hello-dotfiles.html",
    "date"     : "October 2, 2020",
    "excerpt"  : "\nSo I tried Prezto for a couple of months as documented in my earlier post. It works well except that it takes like 2 seconds to get my command prompt after opening a new terminal window (on a Macbook Pro 16). Not a deal breaker, but there must be...",
  "content"  : "\nSo I tried Prezto for a couple of months as documented in my earlier post. It works well except that it takes like 2 seconds to get my command prompt after opening a new terminal window (on a Macbook Pro 16). Not a deal breaker, but there must be something more efficient out there.\n\n\npowerline, dotfiles\n\n\nScouring through the Internet, I finally settled on writing (plagiarizing) my own zsh config files and powerline (cos I still want a fancy prompt).\n\n\n\n\nstow\n\n\nAnd then I found this dotfiles project on Github. It uses GNU stow to manage a hierarcy of dotfiles. The solution is so simple and elegant, it brings tears to my eyes.\n\n\n\n\nyay\n\n\nHere&amp;#8217;s my resulting dotfiles repo. I think it&amp;#8217;ll keep me happy for a while.\n\n\n"
} ,
  
  {
    "title"    : "Podman (v2) on the Mac, in a hurry",
    "category" : "",
    "tags"     : " containers, podman, macos",
    "url"      : "/blog/2020/07/27/podman2-on-mac.html",
    "date"     : "July 27, 2020",
    "excerpt"  : "\nPodman v2 was announced recently. With Podman v2, there are breaking changes to remote client interaction. This post documents how I setup it up on a Mac.\n\n\ntl;dr\n\n\n\n\nInstall podman, vagrant and virtualbox (I suggest via Homebrew)\n\n\n\nIf you have ...",
  "content"  : "\nPodman v2 was announced recently. With Podman v2, there are breaking changes to remote client interaction. This post documents how I setup it up on a Mac.\n\n\ntl;dr\n\n\n\n\nInstall podman, vagrant and virtualbox (I suggest via Homebrew)\n\n\n\nIf you have an older version of podman installed, be sure to upgrade it to version 2.0.x\n\n\n\n\n\nClone/download from https://github.com/naikoob/vagrant-podhost.git\n\n\nBring VM up (vagrant up)\n\n\nAdd alias podman=podman --identity=&amp;lt;path/to/ssh/key&amp;gt; --url=ssh://root@192.168.133.10/run/podman/podman.sock\n\n\n\n\n$ brew cask install podman vagrant virtualbox\n$ git clone https://github.com/naikoob/vagrant-podhost.git\n$ cd vagrant-podhost\n$ vagrant up\n$ echo &quot;alias podman=&#39;podman --identity=&amp;lt;path/to/ssh/key&amp;gt; --url=ssh://root@192.168.133.10/run/podman/podman.sock&#39;&quot; &amp;gt;&amp;gt; ~/.zshrc\n$ source ~/.zshrc\n\n\n\n\n\n\nTry it out:\n\n\n\n\n$ podman run -d -rm -p 8080:80 nginx\n$ curl http://192.168.133.10:8080/\n\n\n\n\n\n\n\n\n\n\npodman\n\n\nPodman is a drop-in replacement for Docker to build and run containers. It&amp;#8217;s daemonless, so you do not need a long running service (Docker daemon), and it supports a Pod concept, so it&amp;#8217;s easier to migrate to Kubernetes when you&amp;#8217;re ready to bring your containerised project into production. Update: Version 2 also understands Kubernetes Deployments\n\n\npodman on a mac\n\nThe macOS does not support containers natively. Running containers on a Mac involves spinning up a Linux virtual machine and interact with the (Linux) container engine using client tools. With Docker, there are tools like Docker Toolbox and Docker Desktop that makes this transparent to the users. Such tools are not available for Podman v2 (&amp;#8230;&amp;#8203;yet). However, it&amp;#8217;s pretty easy to provision a Fedora VM to act as the container/pod host.\n\n\n\n\n\npreparing the host\n\n\nI&amp;#8217;m using vagrant + virtualbox to provision and run my Linux host. Both can be installed via Homebrew:\n\n\n\n$ brew cask install virtualbox vagrant\n\n\n\nI&amp;#8217;m using Fedora as the pod host, as it&amp;#8217;s the easiest way to get the lastest podman binaries. I first provision a Fedora 32 vagrant box and dnf install podman.\n\n\nPodman provides a pair of systemd unit files for API access. Let&amp;#8217;s have a look these systemd unit files:\n\n\n\n# /lib/systemd/system/podman.socket\n[Unit]\nDescription=Podman API Socket\nDocumentation=man:podman-system-service(1)\n\n[Socket]\nListenStream=%t/podman/podman.sock (1)\nSocketMode=0660\n\n[Install]\nWantedBy=sockets.target\n\n\n\n\n\n1\nspecifies the location of the listening socket file. (/run/podman/podman.sock when run as root, or /run/user/&amp;lt;UID&amp;gt;/podman/podman.sock when running as normal user). When a request arrives at this socket, systemd will fire up podman.service below:\n\n\n\n\n\n# /lib/systemd/system/podman.service\n[Unit]\nDescription=Podman API Service\nRequires=podman.socket\nAfter=podman.socket\nDocumentation=man:podman-system-service(1)\nStartLimitIntervalSec=0\n\n[Service]\nType=simple\nExecStart=/usr/bin/podman system service (1)\n\n[Install]\nWantedBy=multi-user.target\nAlso=podman.socket\n\n\n\n\n\n1\nThe command podman system service starts the API service (man podman-system-service).\n\n\n\n\nBy default, podman system service will timeout in 5 seconds (although the man page says 1 second) and exit. I found that this will terminate the containers as well. I&amp;#8217;ll disable the timeout to workaround this behaviour (bug?) by adding the -t 0 arguments:\n\n\n\n$ sudo cp /lib/systemd/system/podman.service /etc/systemd/system/podman.service\n$ sudo sed -i &quot;s/^ExecStart=.*/ExecStart=\\/usr\\/bin\\/podman system service -t 0/&quot; /etc/systemd/system/podman.service\n\n\n\n/etc/systemd/system/podman.service should look like this:\n\n\n\n[Unit]\nDescription=Podman API Service\nRequires=podman.socket\nAfter=podman.socket\nDocumentation=man:podman-system-service(1)\nStartLimitIntervalSec=0\n\n[Service]\nType=simple\nExecStart=/usr/bin/podman system service -t 0\n\n[Install]\nWantedBy=multi-user.target\nAlso=podman.socket\n\n\n\nNow, enable the podman.socket\n\n\n\n$ sudo systemctl enable --now podman.socket\n\n\n\nremote, as in from another IP address&amp;#8230;&amp;#8203;\n\nAs the listening socket is on the host&amp;#8217;s filesystem (/var/run/podman/podman.sock), we&amp;#8217;ll access it remotely via SSH. Append your public key (usually ~/.ssh/id_rsa.pub) on your Mac to the /root/.ssh/authorized_keys file on the virtual machine. Ensure the SSH configuration directory and file has the right permission:\n\n\n\n$ sudo chmod 700 /root/.ssh\n$ sudo chmod 600 /root/.ssh/authorized_keys\n\n\n\n\n\n\nlet&amp;#8217;s podman\n\n\nWe still need the podman-remote client. Easiest way to get it is via Homebrew:\n\n\n\n$ brew cask install podman\n\n\n\nOnce installed, we can invoke podman like so:\n\n\n\n$ podman --identity=&amp;lt;/path/to/ssh/key&amp;gt; --url=ssh://root@podhost/run/podman/podman.sock ps\n\n\n\nTo save some typing, create an alias, for example\n\n\n\n$ echo &quot;alias podman=&#39;podman --identity=&amp;lt;/path/to/ssh/key&amp;gt; --url=ssh://root@192.168.133.10/run/podman/podman.sock&#39;&quot; &amp;gt;&amp;gt; ~/.zshrc\n$ source ~/.zshrc\n\n\n\nNow we can test out with an nginx image:\n\n\n\n$ podman -d -rm -p 8080:80 nginx\n$ curl http://192.168.133.10:8080\n\n\n\n\n\nthat&amp;#8217;s a lot of work, let&amp;#8217;s automate!\n\n\nThe Vagrantfile with provisioning scripts are available on github. Just clone/download and follow the README.\n\n\n\n\nsome fine tuning\n\n\nA couple of things we can do to fine tune the setup:\n\n\n\n\nAdd an entry to /etc/hosts so we can use an intuitive hostname (e.g. podhost) instead of an IP address\n\n\n\n\n# on the mac\n$ sudo echo &quot;192.168.133.10    podhost&quot; &amp;gt;&amp;gt; /etc/hosts\n\n\n\n\n\n\nAdd SSH configuration to specify the user and key file when accessing the podhost. Add the following to ~/.ssh/config (create one if necessary):\n\n\n\n\n# this assumes podhost entry above has been added to /etc/hosts\nHost Podhost\n   HostName podhost\n   User root\n   Identityfile ~/.ssh/id_rsa\n\n\n\n\n\n\n\n\nWith the above, we can simplify the podman command (and the alias) to:\n\n\n\npodman --url=ssh://root@podhost/run/podman/podman.sock\n\n\n\n"
} ,
  
  {
    "title"    : "{AWS} infrastructure as {Ansible} code",
    "category" : "",
    "tags"     : " ansible, aws",
    "url"      : "/blog/2020/07/03/ansible-aws-vpc.html",
    "date"     : "July 3, 2020",
    "excerpt"  : "\nI need to stand up (and tear down) AWS VPCs to try a few OpenShift installation scenarios. I also wanted a revision on Ansible. So I built a VPC with Ansible.\n\n\ntl;dr\n\n\n\n\nClone/download from https://github.com/naikoob/ansible-aws-vpc.git\n\n\nEdit &amp;...",
  "content"  : "\nI need to stand up (and tear down) AWS VPCs to try a few OpenShift installation scenarios. I also wanted a revision on Ansible. So I built a VPC with Ansible.\n\n\ntl;dr\n\n\n\n\nClone/download from https://github.com/naikoob/ansible-aws-vpc.git\n\n\nEdit &amp;lt;repo_root&amp;gt;/inventory/group_vars/aws_infra.yml with AWS account and profile information\n\n\nProvision/unprovision with:\n\n\n\n\n# provision\nansible-playbook -i inventory/base_infra provision_infra.yml\n\n# unprovision\nansible-playbook -i inventory/base_infra provision_infra.yml -e infra_state=absent\n\n\n\n\n\n\nwhen in doubt, check out the README and the comments in the playbooks.\n\n\n\n\n\n\nwhy? what?\n\n\nThis is my seed project for the demos and PoCs I deploy on AWS. I like to automate the provisioning (and unprovisioning) so that I can get the demos/PoCs up and running quickly, but don&amp;#8217;t incur unnecessary costs when not in use.\n\n\nIMHO, if RTO allows, infrastructure as code is the most cost effective disaster recovery mechanism, and any non-trivial IT infrastructure should be deployed in this way.\n\n\nHere&amp;#8217;s what my playbook provisions:\n\n\n\n\nA VPC\n\n\nRoute 53 private hosted zone associated with the VPC\n\n\n3 public subnets across 3 AZs\n\n\n3 private subnets across 3 AZs\n\n\nInternet Gateway and associated route table\n\n\nNAT Gateway and associated route table\n\n\n\n\n\n\nansible . aws . boto\n\n\nAnsible is available on most Linux distributions, so you can install it with apt/dnf/yum/pacman etc. On the Mac, the recommended installation is via Python&amp;#8217;s package manager pip, but I use Homebrew:\n\n\n\nbrew install ansible\n\n\n\nThis should install Ansible and it&amp;#8217;s dependencies, including Python.\n\n\nOne of the strength of Ansible is the extensive collection of modules, including a comprehensive list of AWS modules. Most (all?) Ansible modules for AWS uses the AWS&amp;#8217;s boto3/boto SDK. So we install them with Python&amp;#8217;s package manager pip:\n\n\n\npip install boto3\n\n\n\nFinally, you should have your AWS profiles and credentials configured (~/.aws/config and ~/.aws/credentials) for your account.\n\n\n\n\nansible files\n\n\nNow, clone/download my repository - https://github.com/naikoob/ansible-aws-vpc.git\n\n\naccount variables\n\nI&amp;#8217;ve chosen to embed account and profile information (note: not keys!) in vars files instead of environment variables to minimize the chance of running the playbooks against the wrong account.\n\n\nUpdate /inventory/group_vars/aws_infra.yml with AWS account and profile information:\n\n\n\n# group_vars/aws_infra.yml\n# ---\n# specify infrastructure details and AWS credentials\n\n# aws account, role, profile and region\naws_account: &quot;&amp;lt;AWS_ACCOUNT_NUMBER&amp;gt;&quot;\naws_role: &quot;&amp;lt;AWS_ROLE&amp;gt;&quot;\naws_profile: &quot;&amp;lt;AWS_PROFILE&amp;gt;&quot;                # assumed profile used for provisioning\naws_source_profile: &quot;&amp;lt;AWS_SOURCE_PROFILE&amp;gt;&quot;  # profile used to assume role\naws_region: &quot;&amp;lt;AWS_REGION&amp;gt;&quot;\n\n# truncated ...\n\n\n\nNotice the aws_source_profile variable. This is because I&amp;#8217;ve centralized my IAM users in my main account, and use role switching to access the other accounts (as mentioned in my earlier post).\n\n\nFor example, if my profile is defined as such in ~/.aws/config:\n\n\n\n[profile demo]\nregion=ap-southeast-1\nrole_arn=arn:aws:iam::112233445566:role/DemoAdminRole\nsource_profile=main\n\n\n\nthen the variables should be:\n\n\n\naws_account: &quot;112233445566&quot;\naws_role: &quot;DemoAdminRole&quot;\naws_profile: &quot;demo&quot;                # assumed profile used for provisioning\naws_source_profile: &quot;main&quot;         # profile used to assume role\n\n\n\n\nvpc variables\n\nNext, review and customize the VPC configuration in inventory/host_vars/demo_vpc.yml if necessary.\n\n\n\n# host_vars/vpc.yml\n\n# general details\nvpc_name: demo.vpc\nvpc_dns_zone: demo.example.com.private\n\n# CIDR block\nvpc_cidr_block: 10.0.0.0/16\n\n# Private subnets\nvpc_private_subnets:\n  - { cidr: &quot;10.0.21.0/24&quot;, az: &quot;{{ aws_region }}a&quot; }\n  - { cidr: &quot;10.0.22.0/24&quot;, az: &quot;{{ aws_region }}b&quot; }\n  - { cidr: &quot;10.0.23.0/24&quot;, az: &quot;{{ aws_region }}c&quot; }\n\nvpc_public_subnets:\n  - { cidr: &quot;10.0.201.0/24&quot;, az: &quot;{{ aws_region }}a&quot; }\n  - { cidr: &quot;10.0.202.0/24&quot;, az: &quot;{{ aws_region }}b&quot; }\n  - { cidr: &quot;10.0.203.0/24&quot;, az: &quot;{{ aws_region }}c&quot; }\n\n\n\n\nplaybooks\n\nI&amp;#8217;ve implemented 2 playbooks as follows:\n\n\n\n\nprovision_infra.yml\n\n\n\nThis playbook provision/unprovision the VPC and associated resources (set variable infra_state=absent to unprovision).\n\n\nTo provision:\n\n\n\nansible-playbook -i inventory/base_infra provision_infra.yml\n\n\n\nTo unprovision:\n\n\n\nansible-playbook -i inventory/base_infra provision_infra.yml -e infra_state=absent\n\n\n\n\n\n\ndelete_nat_gateway.yml\n\n\n\nThis playbook deletes the NAT gateway and releases the associated EIP when not in use (to save $$). To re-create the NAT gateway, just re-run the provision_infra.yml playbook. Note that this playbook looks for the var file out/nat_gateway_id.var that is created during VPC provisioning to know which NAT gateway should be deleted. If you&amp;#8217;re deleting NAT gateway from a different machine/directory, you should set the nat_gateway_id variable in a different manner, say on the command line (-e), like so:\n\n\n\nansible-playbook -i inventory/base_infra delete_nat_gateway.yml -e nat_gateway_id=&amp;lt;NAT_GATEWAY_ID&amp;gt;\n\n\n\n\n\n\n\n\n\n\n\nI can, should I?\n\n\nIf you look into the list of tasks, you&amp;#8217;ll see that using Ansible to provision infrastructure on AWS is pretty granular. You provision the individual components that make up your VPC [1]. To me, this is a good thing, as I like to understand what&amp;#8217;s going on under the covers. For others, you may look at provisioning tools like Terraform, and drive the automation with Ansible Terraform module. Maybe I&amp;#8217;ll write about this in a future post ;-).\n\n\n\n\n\n\n1. There used to be an ec2_vpc module in Ansible, but it&amp;#8217;s deprecated and removed since version 2.5\n\n"
} ,
  
  {
    "title"    : "Working with multiple AWS accounts on the command line",
    "category" : "",
    "tags"     : " aws, command line, cloud, security",
    "url"      : "/blog/2020/06/20/aws-cmdline.html",
    "date"     : "June 20, 2020",
    "excerpt"  : "\nIt is a good practice to organise AWS workloads in multiple accounts, be it for security, accounting, or just to maintain our sanity. Here is how I work with multiple accounts in my AWS Organization from the command line.\n\n\naccount(s)\n\n\nWhy does ...",
  "content"  : "\nIt is a good practice to organise AWS workloads in multiple accounts, be it for security, accounting, or just to maintain our sanity. Here is how I work with multiple accounts in my AWS Organization from the command line.\n\n\naccount(s)\n\n\nWhy does AWS encourage multiple accounts setups (apart from getting customers further entrenched on the platform)? This video(3:20-7:00) gives a good explanation.\n\n\nAs for me, I&amp;#8217;m organising my demos and PoCs in a separate Demo account. This way, I can share admin access to my demos with my colleagues without exposing my main account. In addition, I&amp;#8217;m also centralising all IAM users in my main account, and access my Demo (and other accounts) by switching (assuming) roles. For readers new to this setup, I suggest this tutorial.\n\n\nIn my Demo account, I create DemoAdminRole role, with the following trust policy to allow users from my main account to assume this roles:\n\n\n\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Principal&quot;: {\n        &quot;AWS&quot;: &quot;arn:aws:iam::&amp;lt;MAIN_ACCOUNT_NUM&amp;gt;:root&quot;\n      },\n      &quot;Action&quot;: &quot;sts:AssumeRole&quot;\n    }\n  ]\n}\n\n\n\nIn my main account, I created proxy roles that do not have any permission, other then to assume roles in my Demo account, with policy that looks like this:\n\n\n\n{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;sts:AssumeRole&quot;,\n        &quot;Resource&quot;: &quot;arn:aws:iam::&amp;lt;DEMO_ACCOUNT_NUM&amp;gt;:role/&amp;lt;DEMO_ROLE_NAME&amp;gt;&quot;\n    }\n}\n\n\n\nI can then attach this role to users that I want to grant access to my Demo account.\n\n\nFinally, to prevent creation of users and access keys in the Demo account, I attached a service control policy like so to my Demo account:\n\n\n\n{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Effect&quot;: &quot;Deny&quot;,\n            &quot;Action&quot;: [\n                &quot;iam:CreateUser&quot;,\n                &quot;iam:CreateAccessKey&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;*&quot;\n            ]\n        }\n    ]\n}\n\n\n\n\n\nprofiles, profiles\n\n\nAWS CLI and SDKs make use of ~/.aws/config and ~/.aws/credentials to manage AWS profiles and credentials. My ~/.aws/config file looks like this:\n\n\n\n[profile demo]\nregion=ap-southeast-1\nrole_arn=arn:aws:iam::&amp;lt;DEMO_ACCT_NUM&amp;gt;:role/&amp;lt;DEMO_ROLE_NAME&amp;gt;\nsource_profile=demo\n\n[profile main]\nregion=us-west-2\noutput=json\n\n\n\nand ~/.aws/credentials looks like this:\n\n\n\n[demo]\naws_access_key_id     = &amp;lt;DEMO_PROXY_USER_ACCESS_KEY&amp;gt;\naws_secret_access_key = &amp;lt;DEMO_PROXY_USER_SECRET_KEY&amp;gt;\n\n[main]\naws_access_key_id     = &amp;lt;MAIN_ACCESS_KEY&amp;gt;\naws_secret_access_key = &amp;lt;MAIN_SECRET_KEY&amp;gt;\n\n\n\n\n\nswitching between profiles\n\n\nI deliberately not have a default profile to reduce the chance of targeting my scripts and ansible playbooks at the wrong profile. Instead, I added the following.[1] to my .zshrc (and .bashrc) to allow me to quickly switch between profiles:\n\n\n\n# working with AWS profiles\nfunction _aws_list_all {\n    credentialFileLocation=${AWS_SHARED_CREDENTIALS_FILE};\n    if [ -z $credentialFileLocation ]; then\n        credentialFileLocation=~/.aws/credentials\n    fi\n\n    while read line; do\n        if [[ $line == &quot;[&quot;* ]]; then echo &quot;$line&quot;; fi;\n    done &amp;lt; $credentialFileLocation;\n};\n\nfunction _aws_switch_profile() {\n   if [ -z $1 ]; then  echo &quot;Usage: aws-profile profilename&quot;; return; fi\n\n   exists=&quot;$(aws configure get aws_access_key_id --profile $1)&quot;\n   if [[ -n $exists ]]; then\n       export AWS_DEFAULT_PROFILE=$1;\n       export AWS_PROFILE=$1;\n       export AWS_REGION=$(aws configure get region --profile $1);\n       echo &quot;Switched to AWS Profile: $1&quot;;\n       aws configure list\n   fi\n};\n\nalias aws-all=&quot;_aws_list_all&quot;\nalias aws-profile=&quot;_aws_switch_profile&quot;\nalias aws-whoami=&quot;aws configure list&quot;\n\n\n\n&amp;#160;\nI can then switch profiles using the 3 aliases like so:\n\n\n\n\n\n\n\n\n\n\n\n1. I got this off the Internet a while back, but can no longer find the source to give proper credit\n\n"
} ,
  
  {
    "title"    : "On zsh, prezto and nerd fonts...",
    "category" : "",
    "tags"     : " terminal, zsh, prezto, iterm2, command line",
    "url"      : "/blog/2020/06/10/my-terminal-setup.html",
    "date"     : "June 10, 2020",
    "excerpt"  : "\nYay! So I switched job and got a new MacBook Pro. Good excuse to refresh my development (and terminal) setup. I have been using bash along with completion and git-prompt for as long as I can remember, this time I decided to cast my net wider and ...",
  "content"  : "\nYay! So I switched job and got a new MacBook Pro. Good excuse to refresh my development (and terminal) setup. I have been using bash along with completion and git-prompt for as long as I can remember, this time I decided to cast my net wider and see what that world has been up to since I last spend time on this.\n\n\nzsh, prezto\n\n\nI&amp;#8217;m sticking to iTerm2 as my preferred terminal application. As for the shell, I decided to take a look at zsh, seeing that it&amp;#8217;s now the default shell with MacOS Catalina. I also wanted capabilities such as auto-completion, git integration - without a lot of effort ;-). After a bit of googling and reading, I picked prezto as my zsh configuration framework.\n\n\n\n\ninstalling the tools\n\n\nInstalling iterm2 is simple with homebrew:\n\n\n\nbrew cask install iterm2\n\n\n\nZsh that comes with MacOS Catalina is quite current (v5.7.1), so I&amp;#8217;ll just use the OS supplied version.\n\n\nNext, install prezto with the following commands:\n\n\n\ngit clone --recursive https://github.com/sorin-ionescu/prezto.git &quot;${ZDOTDIR:-$HOME}/.zprezto&quot;\n\n\n\nAlternatively, you can install from my fork which contains my modified theme:\n\n\n\ngit clone --recursive https://github.com/naikoob/prezto.git &quot;${ZDOTDIR:-$HOME}/.zprezto&quot;\n\n\n\nfollowed by\n\n\n\nsetopt EXTENDED_GLOB\nfor rcfile in &quot;${ZDOTDIR:-$HOME}&quot;/.zprezto/runcoms/^README.md(.N); do\n  ln -s &quot;$rcfile&quot; &quot;${ZDOTDIR:-$HOME}/.${rcfile:t}&quot;\ndone\n\n\n\n\n\ntinkering\n\n\nFirst, edit the modules section of ~/.zpreztorc to enable the desired modules. I&amp;#8217;ve added a couple of modules as highlighted below:\n\n\n\n# Set the Prezto modules to load (browse modules).\n# The order matters.\nzstyle &#39;:prezto:load&#39; pmodule \\\n  &#39;environment&#39; \\\n  &#39;terminal&#39; \\\n  &#39;editor&#39; \\\n  &#39;history&#39; \\\n  &#39;directory&#39; \\\n  &#39;spectrum&#39; \\\n  &#39;utility&#39; \\\n  &#39;ssh&#39; \\ (1)\n  &#39;completion&#39; \\\n  &#39;git&#39; \\ (2)\n  &#39;syntax-highlighting&#39; \\ (3)\n  &#39;history-substring-search&#39; \\ (4)\n  &#39;prompt&#39;\n\n\n\n\n\n1\nuseful if you have passphrase protected your keys and/or often ssh over a bastion hosts\n\n\n2\nshows git information on your prompt\n\n\n3\neh&amp;#8230;&amp;#8203; syntax highlighting? Well, actually this also does syntax checking as you type\n\n\n4\nenable searching through command history using ctrl-r\n\n\n\n\n\n\nthemes\n\n\nTbh, this is a key motivation for me to explore prezto&amp;#8230;&amp;#8203; The fancy prompt don&amp;#8217;t make me a better developer, but it sure makes me look the part ;-)\n\n\nThis is how my prompt looks like:\n\n\n\n\n\n\n\nIt&amp;#8217;s based on the paradox theme that comes with the prezto distribution, and available on my fork here\n\n\nTo use this theme, copy it to ~/zprezto/modules/prompt/functions/prompt_doxxed_setup, and edit ~/.zpreztorc like so:\n\n\n\n# Set the prompt theme to load.\n# Setting it to &#39;random&#39; loads a random theme.\n# Auto set to &#39;off&#39; on dumb terminals.\nzstyle &#39;:prezto:module:prompt&#39; theme &#39;doxxed&#39;\n\n\n\n\n\nnerd fonts and glyphs\n\n\nNotice the branch and chevron symbols in the screenshot above? Those are not in the standard character set, we need a patched/nerd font so that they display properly. You can find patched fonts here or  here. I use the MesloLGS font from here. Just download and install the .ttf files.\n\n\nIf you&amp;#8217;re only using zsh from iTerm2 exclusively, you can skip the fonts and just enable built-in Powerline glyphs in iTerm2, like so:\n\n\n\n\n\n\n\nAnd while you&amp;#8217;re at it, you can change the colour presets as well. I&amp;#8217;m using &#39;Tango Dark&#39; for the screenshot above.\n\n\n\n\n\n\n\nI use iTerm2&amp;#8217;s built-in glyphs in iTerm2 as I feel they render better, and patched/nerd font in Visual Studio Code terminals. Overall, I&amp;#8217;m pretty satisfied with this setup. Hope you find it useful too.\n\n\n&amp;#160;\n\n\n"
} 
  
  
  
]
